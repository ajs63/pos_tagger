{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk   #import required libraries\n",
    "from nltk.tag import hmm\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus.reader import TaggedCorpusReader \n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "from sklearn import metrics  #for evaluation purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used nltk tagged corpus reader to read training data (http://www.nltk.org/_modules/nltk/corpus/reader/tagged.html)\n",
    "\n",
    "tagged_corpus = TaggedCorpusReader(\".\",\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting  tagged sentences and words from our corpus\n",
    "tagged_sentences = tagged_corpus.tagged_sents()\n",
    "tagged_words = tagged_corpus.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "('The', 'AT')\n",
      "No. of tagged sentences for training:  10388\n",
      "No. of tagged words for training:  228333\n"
     ]
    }
   ],
   "source": [
    "# Peek at the tagged corpus\n",
    "print (tagged_sentences[0])\n",
    "print (tagged_words[0])\n",
    "print(\"No. of tagged sentences for training: \" , len(tagged_sentences))\n",
    "print(\"No. of tagged words for training: \" , len(tagged_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 29090), ('IN', 23657), ('AT', 19840), ('JJ', 12254)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frequency distribution of tags in our data\n",
    "tag_fd = nltk.FreqDist(tag for (word, tag) in tagged_words)\n",
    "tag_fd.most_common(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9349"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# divide the data into train and dev set with ratio of split = 10%\n",
    "split = int(len(tagged_sentences) * 0.9)\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = tagged_sentences[:split]\n",
    "dev_sents = tagged_sentences[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram accuracy: 85.17705927636644\n"
     ]
    }
   ],
   "source": [
    "#Training and testing unigram tagger\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)  \n",
    "print(\"Unigram accuracy:\", unigram_tagger.evaluate(dev_sents)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram accuracy: 86.70900692840647\n",
      "Trigram accuracy: 86.59353348729792\n"
     ]
    }
   ],
   "source": [
    "#Using backoff tagger for bi-tri tagger\n",
    "bigram_tagger = nltk.BigramTagger(train_sents, backoff=unigram_tagger)  \n",
    "print(\"Bigram accuracy:\",bigram_tagger.evaluate(dev_sents)*100)\n",
    "trigram_tagger = nltk.TrigramTagger(train_sents, backoff=bigram_tagger)\n",
    "print(\"Trigram accuracy:\",trigram_tagger.evaluate(dev_sents)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM train accuracy: 97.76034948827049\n",
      "HMM dev accuracy: 43.645111624326404\n"
     ]
    }
   ],
   "source": [
    "#Training and tagging HMM using nltk trainer\n",
    "hmm_trainer = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = hmm_trainer.train_supervised(train_sents)\n",
    "print(\"HMM train accuracy:\", hmm_tagger.evaluate(train_sents)*100)\n",
    "print(\"HMM dev accuracy:\", hmm_tagger.evaluate(dev_sents)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy, Precision, Recall, F1-score. \n",
    "* Precision = True Positive / (True Positive + False Positive) \n",
    "* Recall    = True Positive / (True Positive + False Negative)          \n",
    "* F1-Score = 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unigram Accuracy : 0.8513471901462664\n",
      " Unigram Precision: 0.9124731025455544\n",
      " Unigram Recall   : 0.8513471901462664\n",
      " Unigram F1-Score : 0.8748561520066922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Trigram Accuracy : 0.8654734411085451\n",
      " Trigram Precision: 0.9266259342940507\n",
      " Trigram Recall   : 0.8654734411085451\n",
      " Trigram F1-Score : 0.8915913508772109\n",
      " Hmm Accuracy : 0.43645111624326405\n",
      " Hmm Precision: 0.8727021005868528\n",
      " Hmm Recall   : 0.43645111624326405\n",
      " Hmm F1-Score : 0.5141230809558233\n"
     ]
    }
   ],
   "source": [
    "#Using tags for given corpus to evaluate different metrics for different tagging model\n",
    "uni_dev_tagged_sents = unigram_tagger.tag_sents([[word for word,tag in sentence] for sentence in dev_sents])\n",
    "standard = [str(tag) for sentence in dev_sents for token,tag in sentence]\n",
    "uni_predicted = [str(tag) for sentence in uni_dev_tagged_sents for token,tag in sentence]\n",
    "print(\" Unigram Accuracy :\", metrics.accuracy_score(standard,uni_predicted))\n",
    "print(\" Unigram Precision:\", metrics.precision_score(standard,uni_predicted,average='weighted'))\n",
    "print(\" Unigram Recall   :\", metrics.recall_score(standard,uni_predicted,average='weighted'))\n",
    "print(\" Unigram F1-Score :\", metrics.f1_score(standard,uni_predicted,average='weighted'))\n",
    "\n",
    "tri_dev_tagged_sents = trigram_tagger.tag_sents([[word for word,tag in sentence] for sentence in dev_sents])\n",
    "tri_predicted = [str(tag) for sentence in tri_dev_tagged_sents for token,tag in sentence]\n",
    "print(\" Trigram Accuracy :\", metrics.accuracy_score(standard,tri_predicted))\n",
    "print(\" Trigram Precision:\", metrics.precision_score(standard,tri_predicted,average='weighted'))\n",
    "print(\" Trigram Recall   :\", metrics.recall_score(standard,tri_predicted,average='weighted'))\n",
    "print(\" Trigram F1-Score :\", metrics.f1_score(standard,tri_predicted,average='weighted'))\n",
    "\n",
    "hmm_dev_tagged_sents = hmm_tagger.tag_sents([[word for word,tag in sentence] for sentence in dev_sents])\n",
    "hmm_predicted = [str(tag) for sentence in hmm_dev_tagged_sents for token,tag in sentence]\n",
    "print(\" Hmm Accuracy :\", metrics.accuracy_score(standard,hmm_predicted))\n",
    "print(\" Hmm Precision:\", metrics.precision_score(standard,hmm_predicted,average='weighted'))\n",
    "print(\" Hmm Recall   :\", metrics.recall_score(standard,hmm_predicted,average='weighted'))\n",
    "print(\" Hmm F1-Score :\", metrics.f1_score(standard,hmm_predicted,average='weighted'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Much', 'AP'), ('more', 'AP'), ('than', 'IN'), ('shelter', 'NN'), (',', ','), ('housing', 'VBG'), ('symbolizes', None), ('social', 'JJ'), ('status', 'NN'), (',', ','), ('a', 'AT'), ('sense', 'NN'), ('of', 'IN'), ('``', '``'), ('belonging', 'VBG'), (\"''\", \"''\"), (',', ','), ('acceptance', 'NN'), ('within', 'IN'), ('a', 'AT'), ('given', 'VBN'), ('group', 'NN'), ('or', 'CC'), ('neighborhood', 'NN'), (',', ','), ('identification', 'NN'), ('with', 'IN'), ('particular', 'JJ'), ('cultural', 'JJ'), ('values', 'NNS'), ('and', 'CC'), ('social', 'JJ'), ('institutions', 'NNS'), (',', ','), ('feelings', 'NNS'), ('of', 'IN'), ('pride', 'NN'), ('and', 'CC'), ('worth', 'JJ'), (',', ','), ('aspirations', None), ('and', 'CC'), ('hopes', 'NNS'), ('basic', 'JJ'), ('to', 'IN'), ('human', 'JJ'), ('well', 'RB'), ('-', None), ('being', 'BEG'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#Reading test file using nltk plain text reader and storing sentences\n",
    "test_eva_corpus = TaggedCorpusReader(\".\",\"test.tag\")\n",
    "test_corpus  = PlaintextCorpusReader(\".\",\"test.txt\")\n",
    "test_tagged_eva_sents = test_eva_corpus.tagged_sents()\n",
    "test_sents = test_corpus.sents()\n",
    "# Using trigram (best one here) to generate tagged test file.  \n",
    "\n",
    "text_tagged_sents = trigram_tagger.tag_sents(test_sents)\n",
    "print(text_tagged_sents[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate output file with trigram tagged test sentences with word followed by / and its tag\n",
    "with open('test.out', 'w') as fo:\n",
    "    for i, sentence in enumerate(text_tagged_sents):\n",
    "        print(' '.join([word + '/' + str(tag).lower() for word, tag in sentence]), end='\\n\\n', file=fo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
